
* Data Fetching Utilities Collection

This repository stores a collection of scripts I used to fetch data from WWW over the years. They existed for a long time as single-file scripts for a long time and until recently I started to gradually refactor them into this repository for centralized storage. 

Disclaimer: all scripts here are just to make my life easier, and nothing was intended to be used as web crawler or for large-scale data fetching. 

All scripts tested under ~Ubuntu 22.04, Python 3.10~.


** Scripts and Usage Case

*** Flight Price Check

**** Features

- Check prices for the given flight with selenium 
- Send notification via Pushover, with details results shown as image.

**** Usage

#+begin_src
usage: check_aa_flight.py [-h] -f FROM_AIRPORT -t DEST_AIRPORT [-d [DEPART_DATE]] [-r [RETURN_DATE]] [--repeat-interval [REPEAT_INTERVAL]] [--repeat [REPEAT]] [--notify] [--no-export-pkl] [--pkl-save-folder PKL_SAVE_FOLDER]
                          [--no-export-txt] [--txt-save-folder TXT_SAVE_FOLDER] [--no-export-tsv] [--tsv-save-folder TSV_SAVE_FOLDER]

options:
  -h, --help            show this help message and exit
  -f FROM_AIRPORT, --from FROM_AIRPORT, --origin FROM_AIRPORT
                        Origin airport code
  -t DEST_AIRPORT, --to DEST_AIRPORT, --dest DEST_AIRPORT
                        Origin airport code
  -d [DEPART_DATE], --depart [DEPART_DATE]
                        Depart date in format mm/dd/yyyy or natural language
  -r [RETURN_DATE], --return [RETURN_DATE]
                        Return date in format mm/dd/yyyy or natural language (w.r.t. depart_date)
  --repeat-interval [REPEAT_INTERVAL]
                        Used together with --repeat to seach for the next few <interval>
  --repeat [REPEAT]     Used together with --repeat-interval to search for next few dates with interval <interval>
  --notify              Send notification when results are ready
  --no-export-pkl       Do not savee fights info as PKL file
  --pkl-save-folder PKL_SAVE_FOLDER
                        Folder to save pkl files
  --no-export-txt       Do not save fights info as TXT file
  --txt-save-folder TXT_SAVE_FOLDER
                        Folder to save txt files
  --no-export-tsv       Do not save fights info as TSV file
  --tsv-save-folder TSV_SAVE_FOLDER
                        Folder to save tsv files
#+end_src

For example, assuming that I could like to travel on Wed, return the next Tue after my departure, and I wanted to check for the next 10 weeks. The command is as below:

#+begin_src shell 
python3 check_aa_flight.py -f AAA -t BBB -d "next wed" -r "next Tue" --repeat 10 --repeat-interval 7
#+end_src

The script also support to send notification via Pushover API with result details as well as some bird-view statistics. To set up, define the following environment variable

#+begin_src shell
export PUSHOVER_TOKEN=<API_TOKEN>
export PUSHOVER_USER=<USER_KEY>
#+end_src

The notification looks as below, with the details shown as an image attachment.

#+CAPTION: Pushover notification 
[[./images/aa-flight-pushover-notification.png]]

**** Limitations

- Occasionally fail to load the results, so that some hard-coded refresh is used as a workaround. Will fetch the results eventually in most of time, just the process looks nasty
- Only fetch the price on the depart screen, i.e. shows only the lowest price
- Only implemented for AA flights

**** Why not using an API? 

- Google Flight Search API document got 404
- My quick google search gives me only paid APIs
- I don't have a large-volume requests to make, just mimicking human interactions are good enough serving my purpose

*** Wallhaven Fetcher

**** Features

- Use [[https://wallhaven.cc/help/api][Wallhaven API]] to support query and file download
- Implement a Python API wrapper that could be used in other packages as standalone package
- The request is mad EXTREMELY conservative with explicit waiting interval: this is intended

**** Usage

There are three fetcher defined for shortcuts of common fetching needs.

- **DailyFetcher** - used to fetch the top-list and set to run daily
  
  #+begin_src
  usage: run_wallhaven_daily_fetcher.py [-h] [-d] [-f] [-D [DIR]] [-p [PAGE]] [-i [INTERVAL]]
                                        [-C [CATEGORIES]] [-P [PURITIES]] [-l [LOG_FILE]] [-o]
  
  options:
    -h, --help            show this help message and exit
    -d, --download-files  Whether to download image files (Default False)
    -f, --fetch-details   Whether to fetch details info, e.g. includ. tags (Default False)
    -D [DIR], --dir [DIR]
                          Base directory to save wallpaper files, if not given, environment variable
                          WALLHAVEN_DIR or current folder will be used
    -p [PAGE], --page [PAGE]
                          Range of page to dwonload, use dash (-) for range, comma for separation
    -i [INTERVAL], --interval [INTERVAL]
                          Waiting interval between two downloads or requests
    -C [CATEGORIES], --categories [CATEGORIES]
                          Flag for categories (General/Anime/People) in form of 101 (Default 111)
    -P [PURITIES], --purities [PURITIES]
                          Flag for categories (SFW/Sketchy/NSFW) in form of 110 (Default 110)
    -l [LOG_FILE], --log-file [LOG_FILE]
                          Log file to store logging information
    -o, --keep-output     Leave output when set log-file
  
  #+end_src

- **IDFetcher** - used to fetch the information for a given set of wallpaper ids

  #+begin_src 
  usage: fetch_wallhaven_wallpaper_info.py [-h] [-d] [-D [DIR]] [-I [INPUT]] [-i [INTERVAL]]
                                           [-l [LOG_FILE]] [-o]
                                           [wall_ids ...]
  
  positional arguments:
    wall_ids              Waiting interval between two downloads or requests
  
  options:
    -h, --help            show this help message and exit
    -d, --download-files  Whether to download image files (Default False)
    -D [DIR], --dir [DIR]
                          Base directory to save wallpaper files, if not given, environment
                          variable WALLHAVEN_DIR or current folder will be used
    -I [INPUT], --input [INPUT]
                          Input file containing one wallpaper id per line
    -i [INTERVAL], --interval [INTERVAL]
                          Waiting interval between two downloads or requests
    -l [LOG_FILE], --log-file [LOG_FILE]
                          Log file to store logging information
    -o, --keep-output     Leave output when set log-file

  #+end_src
  
- **QueryFetcher** - This should be the most versatile one to support querying and related work. I have not yet got a use case, just leaving it as a placeholder for now.

**** Todos

- [ ] The API wrapper in ~api.py~ is not complete but just found out a full wrapper already existed: [[https://github.com/Goblenus/WallhavenApi/tree/master][WallhavenApi]].
- [ ] Add support of exclude filter to query or daily fetcher
  
*** Photo Tour Summarize (WIP)

- Collect information from different photo tour website and 
- Used a script to invoke ChatGPT to generate parsing code to support new websites

**** Usage 

There are two kinds of websites defined: 


**** 

